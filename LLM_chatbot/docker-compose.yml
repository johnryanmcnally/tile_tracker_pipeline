services:
  # Ollama hosting servie
  ollama_server:
    image: johnryanmcnally/ollama_server:latest
    build:
      context: . # set current directory
      dockerfile: Dockerfile_ollama
      platforms:
          - linux/arm64
    container_name: ollama_server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # Persist Ollama model data to a named volume
    command: ["serve"] # run 'ollama serve' as the main command for this container
    healthcheck: # healthcheck for ollama-init to see if its healthy
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - joya_network # custom network for inter-service communication

  # Pulls and initializes Ollama models
  init-ollama:
    image: johnryanmcnally/ollama_server_init:latest
    build:
      context: .
      dockerfile: Dockerfile_ollama
      platforms:
          - linux/arm64
    container_name: init-ollama
    networks:
      - joya_network
    depends_on:
      ollama_server:
        condition: service_healthy # Ensure Ollama server is running before attempting to pull models
    environment:
      - OLLAMA_HOST=http://ollama_server:11434  # Set the OLLAMA_HOST to point to the ollama service
    
    # Command to pull the necessary models - more logic to add delays for ollama server health
    entrypoint: ["bash", "-c"]
    command: >
      "
      echo '--- Waiting for Ollama server to be fully responsive ---';
      until curl -s http://ollama_server:11434/api/tags > /dev/null; do
        echo 'Ollama server not yet responsive, waiting 5 seconds...';
        sleep 5;
      done;
      echo '--- Ollama server is responsive. Pausing for a few seconds before initializing models... ---';
      sleep 5;
      echo '--- Initializing models ---';
      ollama pull gemma3:1b || { echo 'ERROR: Failed to pull gemma3:1b model.'; exit 1; };
      ollama pull nomic-embed-text || { echo 'ERROR: Failed to pull nomic-embed-text model.'; exit 1; };
      echo '--- Ollama models initialized successfully ---';
      "
    
  # Service for FastAPI application
  joya_api:
    image: johnryanmcnally/joya_api:latest
    build:
      context: .
      dockerfile: Dockerfile_joya_api
      platforms:
          - linux/arm64
    container_name: joya_api
    ports:
      - "8000:8000"
    environment:
      CHROMA_DB_PATH: "./chroma_langchain_db" # Set the CHROMA_DB_PATH environment variable for the FastAPI app
    depends_on:
      init-ollama:
        condition: service_completed_successfully
    networks:
      - joya_network

# Define named volumes for persistent data
volumes:
  ollama_data:

# Define a custom network for the services to communicate with each other
networks:
  joya_network:
    driver: bridge
